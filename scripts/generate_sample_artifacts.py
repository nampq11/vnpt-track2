#!/usr/bin/env python3
"""Generate sample artifacts for testing runtime pipeline

This script creates minimal but functional indices for testing:
- FAISS vector index
- BM25 keyword index  
- Safety vector matrix
- Metadata JSON

Usage:
    uv run python scripts/generate_sample_artifacts.py
"""

import asyncio
import json
from pathlib import Path
import sys

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.models import Chunk, ChunkMetadata, ChunkType
from src.core.config import Config
from src.offline.indexer.faiss_builder import FAISSIndexBuilder
from src.offline.indexer.bm25_builder import BM25IndexBuilder
from src.offline.indexer.safety_builder import SafetyIndexBuilder
from src.brain.llm.services.ollama import OllamaService


async def main():
    print("=" * 70)
    print("GENERATING SAMPLE ARTIFACTS FOR RUNTIME TESTING")
    print("=" * 70)
    
    # Initialize
    config = Config.from_env()
    output_dir = Path(config.offline.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load or create knowledge base
    kb_file = Path("data/knowledge_base.json")
    if kb_file.exists():
        print(f"\n‚úì Loading knowledge base from {kb_file}")
        with open(kb_file, 'r', encoding='utf-8') as f:
            kb_json = json.load(f)
            
        # Handle different formats
        if isinstance(kb_json, dict) and "documents" in kb_json:
            # New format: {"documents": ["text1", "text2", ...]}
            print("   Format: JSON with 'documents' array (string format)")
            knowledge_data = kb_json["documents"]
        elif isinstance(kb_json, list):
            # Legacy format: [{"title": "...", "content": "...", ...}, ...]
            print("   Format: Array of objects")
            knowledge_data = kb_json
        else:
            print("   ‚ö†Ô∏è Unknown format, using sample data")
            knowledge_data = create_sample_knowledge()
    else:
        print(f"\n‚ö†Ô∏è  {kb_file} not found, creating sample data...")
        knowledge_data = create_sample_knowledge()
        # Save for future use
        kb_file.parent.mkdir(parents=True, exist_ok=True)
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge_data, f, ensure_ascii=False, indent=2)
        print(f"‚úì Saved sample knowledge base to {kb_file}")
    
    # Create chunks
    print(f"\nüì¶ Step 1: Creating chunks from {len(knowledge_data)} documents...")
    chunks = create_chunks_from_knowledge(knowledge_data)
    print(f"   ‚úì Created {len(chunks)} chunks")
    
    # Initialize LLM service for embeddings
    print("\nü§ñ Step 2: Initializing LLM service for embeddings...")
    try:
        llm_service = OllamaService(model=config.ollama.model)
        print(f"   ‚úì Using Ollama model: {config.ollama.model}")
    except Exception as e:
        print(f"   ‚úó Failed to initialize Ollama: {str(e)}")
        print("\nüí° Make sure Ollama is running:")
        print("   curl http://localhost:11434/v1/models")
        return 1
    
    # Build FAISS index
    print("\nüîç Step 3: Building FAISS vector index...")
    try:
        faiss_builder = FAISSIndexBuilder(llm_service)
        print("   ‚è≥ Generating embeddings (this may take a minute)...")
        faiss_index, embeddings = await faiss_builder.build(chunks, embedding_dim=-1)
        
        faiss_path = output_dir / "faiss.index"
        faiss_builder.save(str(faiss_path))
        print(f"   ‚úì Saved FAISS index to {faiss_path}")
        print(f"   ‚úì Index size: {len(embeddings)} vectors √ó {embeddings.shape[1]} dimensions")
    except Exception as e:
        print(f"   ‚úó FAISS build failed: {str(e)}")
        return 1
    
    # Build BM25 index
    print("\nüî§ Step 4: Building BM25 keyword index...")
    try:
        bm25_builder = BM25IndexBuilder()
        bm25_index = bm25_builder.build(chunks)
        
        bm25_path = output_dir / "bm25.pkl"
        bm25_builder.save(str(bm25_path))
        print(f"   ‚úì Saved BM25 index to {bm25_path}")
    except Exception as e:
        print(f"   ‚úó BM25 build failed: {str(e)}")
        return 1
    
    # Build Safety index
    print("\nüõ°Ô∏è  Step 5: Building Safety vector index...")
    try:
        safety_builder = SafetyIndexBuilder(llm_service)
        harmful_questions = safety_builder.generate_synthetic_questions()
        print(f"   ‚è≥ Generating {len(harmful_questions)} safety vectors...")
        
        safety_vectors = await safety_builder.build(harmful_questions)
        
        safety_path = output_dir / "safety.npy"
        safety_builder.save(str(safety_path))
        print(f"   ‚úì Saved safety vectors to {safety_path}")
        print(f"   ‚úì Generated {len(harmful_questions)} harmful question vectors")
    except Exception as e:
        print(f"   ‚úó Safety build failed: {str(e)}")
        return 1
    
    # Save metadata
    print("\nüìÑ Step 6: Saving chunk metadata...")
    try:
        metadata_path = output_dir / "metadata.json"
        save_metadata(chunks, metadata_path)
        print(f"   ‚úì Saved metadata to {metadata_path}")
    except Exception as e:
        print(f"   ‚úó Metadata save failed: {str(e)}")
        return 1
    
    # Summary
    print("\n" + "=" * 70)
    print("‚úÖ ARTIFACTS GENERATION COMPLETE")
    print("=" * 70)
    print(f"üìä Total chunks: {len(chunks)}")
    print(f"üîç FAISS vectors: {len(embeddings)}")
    print(f"üõ°Ô∏è  Safety vectors: {len(harmful_questions)}")
    print(f"üìÅ Output directory: {output_dir}")
    print(f"\n‚ú® You can now run runtime tests:")
    print(f"   uv run pytest tests/test_runtime_rag.py -v")
    print(f"   uv run python predict.py --mode test --input data/test.json")
    
    return 0


def create_sample_knowledge():
    """Create sample Vietnamese knowledge base"""
    return [
        {
            "title": "Lu·∫≠t ƒê·∫•t ƒëai 2024",
            "content": """Lu·∫≠t ƒê·∫•t ƒëai nƒÉm 2024 ƒë∆∞·ª£c Qu·ªëc h·ªôi th√¥ng qua ng√†y 18 th√°ng 1 nƒÉm 2024, 
            c√≥ hi·ªáu l·ª±c t·ª´ ng√†y 1 th√°ng 1 nƒÉm 2025. Lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ ch·∫ø ƒë·ªô s·ªü h·ªØu, 
            quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, nghƒ©a v·ª• v√† tr√°ch nhi·ªám c·ªßa ng∆∞·ªùi s·ª≠ d·ª•ng ƒë·∫•t. 
            
            ƒêi·ªÅu 4: Nh√† n∆∞·ªõc th·ªëng nh·∫•t qu·∫£n l√Ω v·ªÅ ƒë·∫•t ƒëai trong ph·∫°m vi c·∫£ n∆∞·ªõc.
            
            ƒêi·ªÅu 10: Ng∆∞·ªùi s·ª≠ d·ª•ng ƒë·∫•t c√≥ quy·ªÅn chuy·ªÉn ƒë·ªïi, chuy·ªÉn nh∆∞·ª£ng, cho thu√™, 
            cho thu√™ l·∫°i, th·ª´a k·∫ø, t·∫∑ng cho quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t theo quy ƒë·ªãnh c·ªßa Lu·∫≠t n√†y.""",
            "year": 2024,
            "type": "LAW",
            "province": "ALL"
        },
        {
            "title": "Hi·∫øn ph√°p 2013",
            "content": """Hi·∫øn ph√°p n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam nƒÉm 2013 
            ƒë∆∞·ª£c Qu·ªëc h·ªôi kh√≥a XIII, k·ª≥ h·ªçp th·ª© 6 th√¥ng qua ng√†y 28 th√°ng 11 nƒÉm 2013.
            
            ƒêi·ªÅu 1: N∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam l√† m·ªôt n∆∞·ªõc ƒë·ªôc l·∫≠p, 
            c√≥ ch·ªß quy·ªÅn, th·ªëng nh·∫•t v√† to√†n v·∫πn l√£nh th·ªï, bao g·ªìm ƒë·∫•t li·ªÅn, h·∫£i ƒë·∫£o, 
            v√πng bi·ªÉn v√† v√πng tr·ªùi.
            
            ƒêi·ªÅu 2: Nh√† n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam l√† Nh√† n∆∞·ªõc ph√°p quy·ªÅn 
            x√£ h·ªôi ch·ªß nghƒ©a c·ªßa nh√¢n d√¢n, do nh√¢n d√¢n, v√¨ nh√¢n d√¢n.""",
            "year": 2013,
            "type": "LAW",
            "province": "ALL"
        },
        {
            "title": "To√°n h·ªçc - ƒê·∫°o h√†m c∆° b·∫£n",
            "content": """ƒê·∫°o h√†m l√† m·ªôt trong nh·ªØng kh√°i ni·ªám c∆° b·∫£n trong gi·∫£i t√≠ch.
            
            ƒê·ªãnh nghƒ©a: ƒê·∫°o h√†m c·ªßa h√†m s·ªë f(x) t·∫°i ƒëi·ªÉm x‚ÇÄ l√† gi·ªõi h·∫°n:
            f'(x‚ÇÄ) = lim[h‚Üí0] (f(x‚ÇÄ+h) - f(x‚ÇÄ))/h
            
            C√°c c√¥ng th·ª©c ƒë·∫°o h√†m c∆° b·∫£n:
            - (c)' = 0 (c l√† h·∫±ng s·ªë)
            - (x^n)' = n¬∑x^(n-1)
            - (sin x)' = cos x
            - (cos x)' = -sin x
            - (e^x)' = e^x
            - (ln x)' = 1/x
            
            Quy t·∫Øc t·ªïng: (f + g)' = f' + g'
            Quy t·∫Øc t√≠ch: (f¬∑g)' = f'¬∑g + f¬∑g'
            Quy t·∫Øc th∆∞∆°ng: (f/g)' = (f'¬∑g - f¬∑g')/g¬≤""",
            "year": 2020,
            "type": "MATH",
            "province": "ALL"
        },
        {
            "title": "V·∫≠t l√Ω - ƒêi·ªán tr·ªü v√† ƒë·ªãnh lu·∫≠t Ohm",
            "content": """ƒêi·ªán tr·ªü l√† ƒë·∫°i l∆∞·ª£ng ƒë·∫∑c tr∆∞ng cho m·ª©c ƒë·ªô c·∫£n tr·ªü d√≤ng ƒëi·ªán 
            c·ªßa v·∫≠t d·∫´n.
            
            C√¥ng th·ª©c t√≠nh ƒëi·ªán tr·ªü: R = œÅ¬∑L/S
            Trong ƒë√≥:
            - R: ƒëi·ªán tr·ªü (Œ©)
            - œÅ: ƒëi·ªán tr·ªü su·∫•t c·ªßa v·∫≠t li·ªáu (Œ©¬∑m)
            - L: chi·ªÅu d√†i d√¢y d·∫´n (m)
            - S: ti·∫øt di·ªán d√¢y d·∫´n (m¬≤)
            
            ƒê·ªãnh lu·∫≠t Ohm: U = I¬∑R
            Trong ƒë√≥:
            - U: hi·ªáu ƒëi·ªán th·∫ø (V)
            - I: c∆∞·ªùng ƒë·ªô d√≤ng ƒëi·ªán (A)
            - R: ƒëi·ªán tr·ªü (Œ©)
            
            C√¥ng su·∫•t ƒëi·ªán: P = U¬∑I = I¬≤¬∑R = U¬≤/R""",
            "year": 2020,
            "type": "MATH",
            "province": "ALL"
        },
        {
            "title": "L·ªãch s·ª≠ - C√°ch m·∫°ng th√°ng T√°m 1945",
            "content": """C√°ch m·∫°ng th√°ng T√°m nƒÉm 1945 l√† cu·ªôc c√°ch m·∫°ng gi·∫£i ph√≥ng d√¢n t·ªôc 
            c·ªßa nh√¢n d√¢n Vi·ªát Nam do ƒê·∫£ng C·ªông s·∫£n ƒê√¥ng D∆∞∆°ng v√† Ch·ªß t·ªãch H·ªì Ch√≠ Minh l√£nh ƒë·∫°o.
            
            B·ªëi c·∫£nh: Sau khi ph√°t x√≠t Nh·∫≠t ƒë·∫ßu h√†ng ƒê·ªìng minh (15/8/1945), 
            t·∫°o ra ch√¢n kh√¥ng quy·ªÅn l·ª±c t·∫°i Vi·ªát Nam.
            
            Di·ªÖn bi·∫øn:
            - 16/8/1945: ƒê·∫°i h·ªôi qu·ªëc d√¢n ·ªü T√¢n Tr√†o quy·∫øt ƒë·ªãnh t·ªïng kh·ªüi nghƒ©a
            - 19/8/1945: Kh·ªüi nghƒ©a gi√†nh ch√≠nh quy·ªÅn ·ªü H√† N·ªôi
            - 23/8/1945: C√°ch m·∫°ng th√†nh c√¥ng t·∫°i Hu·∫ø
            - 25/8/1945: Ch√≠nh quy·ªÅn c√°ch m·∫°ng n·∫Øm S√†i G√≤n
            - 2/9/1945: Ch·ªß t·ªãch H·ªì Ch√≠ Minh ƒë·ªçc Tuy√™n ng√¥n ƒê·ªôc l·∫≠p, 
              tuy√™n b·ªë th√†nh l·∫≠p n∆∞·ªõc Vi·ªát Nam D√¢n ch·ªß C·ªông h√≤a
            
            √ù nghƒ©a: L·∫ßn ƒë·∫ßu ti√™n trong l·ªãch s·ª≠, nh√¢n d√¢n ta gi√†nh ƒë∆∞·ª£c ch√≠nh quy·ªÅn 
            tr√™n ph·∫°m vi c·∫£ n∆∞·ªõc.""",
            "year": 1945,
            "type": "HISTORY",
            "province": "ALL"
        },
        {
            "title": "Kinh t·∫ø - GDP v√† tƒÉng tr∆∞·ªüng kinh t·∫ø",
            "content": """GDP (Gross Domestic Product - T·ªïng s·∫£n ph·∫©m qu·ªëc n·ªôi) l√† 
            t·ªïng gi√° tr·ªã th·ªã tr∆∞·ªùng c·ªßa t·∫•t c·∫£ h√†ng h√≥a v√† d·ªãch v·ª• cu·ªëi c√πng ƒë∆∞·ª£c s·∫£n xu·∫•t 
            trong m·ªôt qu·ªëc gia trong m·ªôt kho·∫£ng th·ªùi gian nh·∫•t ƒë·ªãnh (th∆∞·ªùng l√† m·ªôt nƒÉm).
            
            C√°c ph∆∞∆°ng ph√°p t√≠nh GDP:
            1. Ph∆∞∆°ng ph√°p s·∫£n xu·∫•t: GDP = Œ£ Gi√° tr·ªã gia tƒÉng
            2. Ph∆∞∆°ng ph√°p thu nh·∫≠p: GDP = L∆∞∆°ng + L·ª£i nhu·∫≠n + Thu·∫ø
            3. Ph∆∞∆°ng ph√°p chi ti√™u: GDP = C + I + G + (X - M)
               - C: Ti√™u d√πng c√° nh√¢n
               - I: ƒê·∫ßu t∆∞
               - G: Chi ti√™u ch√≠nh ph·ªß
               - X: Xu·∫•t kh·∫©u
               - M: Nh·∫≠p kh·∫©u
            
            TƒÉng tr∆∞·ªüng GDP = (GDP nƒÉm nay - GDP nƒÉm tr∆∞·ªõc) / GDP nƒÉm tr∆∞·ªõc √ó 100%
            
            GDP b√¨nh qu√¢n ƒë·∫ßu ng∆∞·ªùi = GDP / D√¢n s·ªë""",
            "year": 2020,
            "type": "GENERAL",
            "province": "ALL"
        },
    ]


def create_chunks_from_knowledge(knowledge_data):
    """Convert knowledge base entries to Chunk objects
    
    Supports two formats:
    1. List of objects: [{"title": "...", "content": "...", "year": ..., "type": "..."}]
    2. List of strings: ["text1", "text2", ...]
    """
    chunks = []
    
    for idx, doc in enumerate(knowledge_data):
        chunk_id = f"chunk_{idx:05d}"
        
        # Handle different document formats
        if isinstance(doc, dict):
            # Format 1: Object with title, content, year, type
            text = doc.get("content", "").strip()
            source = doc.get("title", f"doc_{idx}")
            doc_type = doc.get("type", "GENERAL")
            year = doc.get("year", 2020)
            province = doc.get("province", "ALL")
        elif isinstance(doc, str):
            # Format 2: Plain string
            text = doc.strip()
            source = f"doc_{idx}"
            doc_type = "GENERAL"
            year = 2020
            province = "ALL"
        else:
            print(f"   ‚ö†Ô∏è Skipping unknown doc format at index {idx}")
            continue
        
        # Ensure type is valid
        try:
            chunk_type = ChunkType[doc_type]
        except KeyError:
            chunk_type = ChunkType.GENERAL
        
        # Create metadata
        metadata = ChunkMetadata(
            source=source,
            type=chunk_type,
            valid_from=year,
            expire_at=9999,
            province=province
        )
        
        # Create chunk
        chunk = Chunk(
            id=chunk_id,
            text=text,
            metadata=metadata
        )
        
        chunks.append(chunk)
    
    return chunks


def save_metadata(chunks, output_path):
    """Save chunk metadata to JSON for reference"""
    metadata_list = []
    
    for chunk in chunks:
        metadata_list.append({
            "id": chunk.id,
            "text_preview": chunk.text[:150] + "..." if len(chunk.text) > 150 else chunk.text,
            "text_length": len(chunk.text),
            "source": chunk.metadata.source,
            "type": chunk.metadata.type.value,
            "valid_from": chunk.metadata.valid_from,
            "expire_at": chunk.metadata.expire_at,
            "province": chunk.metadata.province,
        })
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(metadata_list, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

